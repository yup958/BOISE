col_sample_size = 100
row_sample_size = 100
inform = c(91, 10, 747, 85, 137, 525, 62, 189, 297, 470, 638)
inform_CID = colnames(train)[inform]
nT = as.integer(ncol(train) * 0.1)
## evaluate function of the top set
Evaluate <-function(cl_sample, inform, measure, percent,
test, train, nT, col_sample_size, row_sample_size, m0_selections){
Scores = rep(0, ncol(train))
complete_idx = which(!is.na(test))
xA = test[inform]
nA = length(inform)
for (k in 1:col_sample_size) {
# load block RData file
if(k == 8){
next
}
load(paste('~/CHTC_Downloads/block_res_', as.character(k), '.RData', sep = ''))
# specific m0 priors
m0_selection = m0_selections[which(m0_selections$V1==k),]
## cluster assignment. Merge all column clusters with size <= 2
cl = cl_sample$CC[k,]
for (grp in 1:cl_sample$KK[k]){
if(cl_sample$NN[k, grp] <= 2)
break
}
cl[which(cl > grp)] = grp
Scores = Scores + evaluate_interm(cl, inform, train, test,
m0_selection, block, row_sample_size)
}
Scores = Scores / (col_sample_size-1)
Scores[inform[which(xA==1)]] = rep(max(Scores) + 1, sum(xA, na.rm = T))
Scores[inform[which(xA==0)]] = rep(min(Scores) - 1, sum(1-xA, na.rm = T))
return(Scores)
# test = as.vector(test[complete_idx])
# Scores = as.vector(Scores[complete_idx])
# if(measure == "nef"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     nTop = round(ncol(train) * percent, 0)
#     top = order(Scores,decreasing = T)[1:nTop]
#     pred_hit = sum(test[top])
#     hit = sum(test)
#     maxhit = min(hit,nTop)
#     result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
#   }
# } else if(measure == 'selectivity_nef'){
#   nTop = round(ncol(train) * percent, 0)
#   top = order(Scores,decreasing = T)[1:nTop]
#   selectivity_score = colMeans(train, na.rm = T)
#   test = test - selectivity_score
#   pred_hit = sum(test[top])
#   hit = sum(test)
#   maxhit = sum(sort(test, decreasing = T)[1:nTop])
#   result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
# } else if(measure == "rocauc"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     rocobj = pROC::roc(response = test, predictor = Scores, quiet = TRUE)
#     result = rocobj$auc
#   }
# } else{
#   print("Criteria is not supported.")
#   result = 0
# }
# return(result)
}
roc_results = read.table('block_roc_results.txt', sep = ' ', header=T)
nA_results = read.table('block_nA_results.txt', sep = ' ', header=T)
#nef_results = read.table('nef_results.txt', sep = ' ', header=T)
cache = list()
for (k in 9:11) {
nA_name = paste('nA_', as.character(k), sep = '')
roc_name = paste('rocauc_', as.character(k), sep = '')
#nef_name = paste('nef_', as.character(k), sep = '')
roc_results[, roc_name] = rep(NA, 100)
#nef_results[, nef_name] = rep(NA, 100)
nA_results[, nA_name] = rep(0, 100)
tmp_inform = inform[1:k]
for (i in 1:100) {
print(i)
if(sum(test[i, ], na.rm = T) == 0){
print('All observed responses are 0!')
next
}
xa = paste(test[i, tmp_inform], collapse = '')
complete_idx = which(!is.na(test[i,]))
valid_inform = intersect(complete_idx, tmp_inform)
nA_results[i, nA_name] = length(valid_inform)
if (length(valid_inform) >= 1){
if(xa %in% names(cache)){
Scores = cache[[xa]]
} else{
Scores = Evaluate(cl_sample, tmp_inform, measure='rocauc', percent=0.1,
test=test[i,], train, nT, col_sample_size, row_sample_size, m0_selections)
cache[[xa]] = Scores
}
Response = as.vector(test[i, complete_idx])
Scores = as.vector(Scores[complete_idx])
rocobj = pROC::roc(response = Response, predictor = Scores, quiet = TRUE)
roc_results[i, roc_name] = rocobj$auc
}
}
}
setwd('~/RAwork/BOISE/BOISE_followup/FDA_analysis/')
source('evaluate_interm.R')
set.seed(2)
load('col_clust_res_5.RData')
m0_selections = read.table('~/CHTC_Downloads/Prior_mass_FDA_separate.txt')
## train-test separation
load('fda_data_rearranged.RData')
block_res$row_block = block_res$row_block[1:5,]
block_res$col_block = block_res$col_block[1:5,]
active_ind = apply(block_res$col_block, 2, sum)
active_ind = which(active_ind > 0)
cpds = colnames(block_res$col_block[, active_ind])
dat = dat[, cpds]
count = apply(dat,1, function(x){return(sum(is.na(x)))})
test_AID = names(sort(count)[1:100])
train_AID = setdiff(rownames(dat), test_AID)
train = dat[train_AID, ]
test = dat[test_AID,]
rm(dat)
col_sample_size = 100
row_sample_size = 100
inform = c(91, 10, 747, 85, 137, 525, 62, 189, 297, 470, 638)
inform_CID = colnames(train)[inform]
nT = as.integer(ncol(train) * 0.1)
## evaluate function of the top set
Evaluate <-function(cl_sample, inform, measure, percent,
test, train, nT, col_sample_size, row_sample_size, m0_selections){
Scores = rep(0, ncol(train))
complete_idx = which(!is.na(test))
xA = test[inform]
nA = length(inform)
for (k in 1:col_sample_size) {
# load block RData file
if(k == 8){
next
}
load(paste('~/CHTC_Downloads/block_res_', as.character(k), '.RData', sep = ''))
# specific m0 priors
m0_selection = m0_selections[which(m0_selections$V1==k),]
## cluster assignment. Merge all column clusters with size <= 2
cl = cl_sample$CC[k,]
for (grp in 1:cl_sample$KK[k]){
if(cl_sample$NN[k, grp] <= 2)
break
}
cl[which(cl > grp)] = grp
Scores = Scores + evaluate_interm(cl, inform, train, test,
m0_selection, block, row_sample_size)
}
Scores = Scores / (col_sample_size-1)
Scores[inform[which(xA==1)]] = rep(max(Scores) + 1, sum(xA, na.rm = T))
Scores[inform[which(xA==0)]] = rep(min(Scores) - 1, sum(1-xA, na.rm = T))
return(Scores)
# test = as.vector(test[complete_idx])
# Scores = as.vector(Scores[complete_idx])
# if(measure == "nef"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     nTop = round(ncol(train) * percent, 0)
#     top = order(Scores,decreasing = T)[1:nTop]
#     pred_hit = sum(test[top])
#     hit = sum(test)
#     maxhit = min(hit,nTop)
#     result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
#   }
# } else if(measure == 'selectivity_nef'){
#   nTop = round(ncol(train) * percent, 0)
#   top = order(Scores,decreasing = T)[1:nTop]
#   selectivity_score = colMeans(train, na.rm = T)
#   test = test - selectivity_score
#   pred_hit = sum(test[top])
#   hit = sum(test)
#   maxhit = sum(sort(test, decreasing = T)[1:nTop])
#   result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
# } else if(measure == "rocauc"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     rocobj = pROC::roc(response = test, predictor = Scores, quiet = TRUE)
#     result = rocobj$auc
#   }
# } else{
#   print("Criteria is not supported.")
#   result = 0
# }
# return(result)
}
roc_results = read.table('block_roc_results.txt', sep = ' ', header=T)
nA_results = read.table('block_nA_results.txt', sep = ' ', header=T)
#nef_results = read.table('nef_results.txt', sep = ' ', header=T)
cache = list()
for (k in 9:11) {
nA_name = paste('nA_', as.character(k), sep = '')
roc_name = paste('rocauc_', as.character(k), sep = '')
#nef_name = paste('nef_', as.character(k), sep = '')
roc_results[, roc_name] = rep(NA, 100)
#nef_results[, nef_name] = rep(NA, 100)
nA_results[, nA_name] = rep(0, 100)
tmp_inform = inform[1:k]
for (i in 1:100) {
print(i)
if(sum(test[i, ], na.rm = T) == 0){
print('All observed responses are 0!')
next
}
xa = paste(test[i, tmp_inform], collapse = '')
complete_idx = which(!is.na(test[i,]))
valid_inform = intersect(complete_idx, tmp_inform)
nA_results[i, nA_name] = length(valid_inform)
if (length(valid_inform) >= 1){
if(xa %in% names(cache)){
Scores = cache[[xa]]
} else{
Scores = Evaluate(cl_sample, tmp_inform, measure='rocauc', percent=0.1,
test=test[i,], train, nT, col_sample_size, row_sample_size, m0_selections)
cache[[xa]] = Scores
}
Response = as.vector(test[i, complete_idx])
Scores = as.vector(Scores[complete_idx])
rocobj = pROC::roc(response = Response, predictor = Scores, quiet = TRUE)
roc_results[i, roc_name] = rocobj$auc
}
}
print(summary(roc_results[, roc_name]))
}
roc_results = read.table('block_roc_results.txt', sep = ' ', header=T)
setwd('~/RAwork/BOISE/BOISE_followup/FDA_analysis/')
source('evaluate_interm.R')
set.seed(2)
load('col_clust_res_5.RData')
m0_selections = read.table('~/CHTC_Downloads/Prior_mass_FDA_separate.txt')
## train-test separation
load('fda_data_rearranged.RData')
block_res$row_block = block_res$row_block[1:5,]
block_res$col_block = block_res$col_block[1:5,]
active_ind = apply(block_res$col_block, 2, sum)
active_ind = which(active_ind > 0)
cpds = colnames(block_res$col_block[, active_ind])
dat = dat[, cpds]
count = apply(dat,1, function(x){return(sum(is.na(x)))})
test_AID = names(sort(count)[1:100])
train_AID = setdiff(rownames(dat), test_AID)
train = dat[train_AID, ]
test = dat[test_AID,]
rm(dat)
col_sample_size = 100
row_sample_size = 100
inform = c(91, 10, 747, 85, 137, 525, 62, 189, 297, 470, 638, 82, 46)
inform_CID = colnames(train)[inform]
nT = as.integer(ncol(train) * 0.1)
## evaluate function of the top set
Evaluate <-function(cl_sample, inform, measure, percent,
test, train, nT, col_sample_size, row_sample_size, m0_selections){
Scores = rep(0, ncol(train))
complete_idx = which(!is.na(test))
xA = test[inform]
nA = length(inform)
for (k in 1:col_sample_size) {
# load block RData file
if(k == 8){
next
}
load(paste('~/CHTC_Downloads/block_res_', as.character(k), '.RData', sep = ''))
# specific m0 priors
m0_selection = m0_selections[which(m0_selections$V1==k),]
## cluster assignment. Merge all column clusters with size <= 2
cl = cl_sample$CC[k,]
for (grp in 1:cl_sample$KK[k]){
if(cl_sample$NN[k, grp] <= 2)
break
}
cl[which(cl > grp)] = grp
Scores = Scores + evaluate_interm(cl, inform, train, test,
m0_selection, block, row_sample_size)
}
Scores = Scores / (col_sample_size-1)
Scores[inform[which(xA==1)]] = rep(max(Scores) + 1, sum(xA, na.rm = T))
Scores[inform[which(xA==0)]] = rep(min(Scores) - 1, sum(1-xA, na.rm = T))
return(Scores)
# test = as.vector(test[complete_idx])
# Scores = as.vector(Scores[complete_idx])
# if(measure == "nef"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     nTop = round(ncol(train) * percent, 0)
#     top = order(Scores,decreasing = T)[1:nTop]
#     pred_hit = sum(test[top])
#     hit = sum(test)
#     maxhit = min(hit,nTop)
#     result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
#   }
# } else if(measure == 'selectivity_nef'){
#   nTop = round(ncol(train) * percent, 0)
#   top = order(Scores,decreasing = T)[1:nTop]
#   selectivity_score = colMeans(train, na.rm = T)
#   test = test - selectivity_score
#   pred_hit = sum(test[top])
#   hit = sum(test)
#   maxhit = sum(sort(test, decreasing = T)[1:nTop])
#   result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
# } else if(measure == "rocauc"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     rocobj = pROC::roc(response = test, predictor = Scores, quiet = TRUE)
#     result = rocobj$auc
#   }
# } else{
#   print("Criteria is not supported.")
#   result = 0
# }
# return(result)
}
roc_results = read.table('block_roc_results.txt', sep = ' ', header=T)
nA_results = read.table('block_nA_results.txt', sep = ' ', header=T)
#nef_results = read.table('nef_results.txt', sep = ' ', header=T)
cache = list()
for (k in 12:13) {
nA_name = paste('nA_', as.character(k), sep = '')
roc_name = paste('rocauc_', as.character(k), sep = '')
#nef_name = paste('nef_', as.character(k), sep = '')
roc_results[, roc_name] = rep(NA, 100)
#nef_results[, nef_name] = rep(NA, 100)
nA_results[, nA_name] = rep(0, 100)
tmp_inform = inform[1:k]
for (i in 1:100) {
print(i)
if(sum(test[i, ], na.rm = T) == 0){
print('All observed responses are 0!')
next
}
xa = paste(test[i, tmp_inform], collapse = '')
complete_idx = which(!is.na(test[i,]))
valid_inform = intersect(complete_idx, tmp_inform)
nA_results[i, nA_name] = length(valid_inform)
if (length(valid_inform) >= 1){
if(xa %in% names(cache)){
Scores = cache[[xa]]
} else{
Scores = Evaluate(cl_sample, tmp_inform, measure='rocauc', percent=0.1,
test=test[i,], train, nT, col_sample_size, row_sample_size, m0_selections)
cache[[xa]] = Scores
}
Response = as.vector(test[i, complete_idx])
Scores = as.vector(Scores[complete_idx])
rocobj = pROC::roc(response = Response, predictor = Scores, quiet = TRUE)
roc_results[i, roc_name] = rocobj$auc
}
}
print(summary(roc_results[, roc_name]))
}
write.table(nA_results, file = 'block_nA_results.txt',row.names = F)
write.table(roc_results, file = 'block_roc_results.txt',row.names = F)
summry(roc_results$rocauc_13)
summary(roc_results$rocauc_13)
summary(roc_results$rocauc_12)
setwd('~/RAwork/BOISE/BOISE_followup/FDA_analysis/')
source('evaluate_interm.R')
set.seed(2)
load('col_clust_res_5.RData')
m0_selections = read.table('~/CHTC_Downloads/Prior_mass_FDA_separate.txt')
## train-test separation
load('fda_data_rearranged.RData')
block_res$row_block = block_res$row_block[1:5,]
block_res$col_block = block_res$col_block[1:5,]
active_ind = apply(block_res$col_block, 2, sum)
active_ind = which(active_ind > 0)
cpds = colnames(block_res$col_block[, active_ind])
dat = dat[, cpds]
count = apply(dat,1, function(x){return(sum(is.na(x)))})
test_AID = names(sort(count)[1:100])
train_AID = setdiff(rownames(dat), test_AID)
train = dat[train_AID, ]
test = dat[test_AID,]
rm(dat)
col_sample_size = 100
row_sample_size = 100
inform = c(91, 10, 747, 85, 137, 525, 62, 189, 297, 470, 638, 82, 46)
inform_CID = colnames(train)[inform]
nT = as.integer(ncol(train) * 0.1)
## evaluate function of the top set
Evaluate <-function(cl_sample, inform, measure, percent,
test, train, nT, col_sample_size, row_sample_size, m0_selections){
Scores = rep(0, ncol(train))
complete_idx = which(!is.na(test))
xA = test[inform]
nA = length(inform)
for (k in 1:col_sample_size) {
# load block RData file
if(k == 8){
next
}
load(paste('~/CHTC_Downloads/block_res_', as.character(k), '.RData', sep = ''))
# specific m0 priors
m0_selection = m0_selections[which(m0_selections$V1==k),]
## cluster assignment. Merge all column clusters with size <= 2
cl = cl_sample$CC[k,]
for (grp in 1:cl_sample$KK[k]){
if(cl_sample$NN[k, grp] <= 2)
break
}
cl[which(cl > grp)] = grp
Scores = Scores + evaluate_interm(cl, inform, train, test,
m0_selection, block, row_sample_size)
}
Scores = Scores / (col_sample_size-1)
Scores[inform[which(xA==1)]] = rep(max(Scores) + 1, sum(xA, na.rm = T))
Scores[inform[which(xA==0)]] = rep(min(Scores) - 1, sum(1-xA, na.rm = T))
return(Scores)
# test = as.vector(test[complete_idx])
# Scores = as.vector(Scores[complete_idx])
# if(measure == "nef"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     nTop = round(ncol(train) * percent, 0)
#     top = order(Scores,decreasing = T)[1:nTop]
#     pred_hit = sum(test[top])
#     hit = sum(test)
#     maxhit = min(hit,nTop)
#     result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
#   }
# } else if(measure == 'selectivity_nef'){
#   nTop = round(ncol(train) * percent, 0)
#   top = order(Scores,decreasing = T)[1:nTop]
#   selectivity_score = colMeans(train, na.rm = T)
#   test = test - selectivity_score
#   pred_hit = sum(test[top])
#   hit = sum(test)
#   maxhit = sum(sort(test, decreasing = T)[1:nTop])
#   result = ((pred_hit/nTop - hit/ncol(train)) / (maxhit/nTop - hit/ncol(train)) + 1)/2
# } else if(measure == "rocauc"){
#   if(sum(test, na.rm = T) == 0){
#     print('All observed responses are 0!')
#     result = NA
#   } else{
#     rocobj = pROC::roc(response = test, predictor = Scores, quiet = TRUE)
#     result = rocobj$auc
#   }
# } else{
#   print("Criteria is not supported.")
#   result = 0
# }
# return(result)
}
roc_results = read.table('block_roc_results.txt', sep = ' ', header=T)
nA_results = read.table('block_nA_results.txt', sep = ' ', header=T)
#nef_results = read.table('nef_results.txt', sep = ' ', header=T)
cache = list()
View(roc_results)
for (k in 9:11) {
nA_name = paste('nA_', as.character(k), sep = '')
roc_name = paste('rocauc_', as.character(k), sep = '')
#nef_name = paste('nef_', as.character(k), sep = '')
roc_results[, roc_name] = rep(NA, 100)
#nef_results[, nef_name] = rep(NA, 100)
nA_results[, nA_name] = rep(0, 100)
tmp_inform = inform[1:k]
for (i in 1:100) {
print(i)
if(sum(test[i, ], na.rm = T) == 0){
print('All observed responses are 0!')
next
}
xa = paste(test[i, tmp_inform], collapse = '')
complete_idx = which(!is.na(test[i,]))
valid_inform = intersect(complete_idx, tmp_inform)
nA_results[i, nA_name] = length(valid_inform)
if (length(valid_inform) >= 1){
if(xa %in% names(cache)){
Scores = cache[[xa]]
} else{
Scores = Evaluate(cl_sample, tmp_inform, measure='rocauc', percent=0.1,
test=test[i,], train, nT, col_sample_size, row_sample_size, m0_selections)
cache[[xa]] = Scores
}
Response = as.vector(test[i, complete_idx])
Scores = as.vector(Scores[complete_idx])
rocobj = pROC::roc(response = Response, predictor = Scores, quiet = TRUE)
roc_results[i, roc_name] = rocobj$auc
}
}
print(summary(roc_results[, roc_name]))
}
write.table(nA_results, file = 'block_nA_results.txt',row.names = F)
write.table(roc_results, file = 'block_roc_results.txt',row.names = F)
View(block_res)
View(roc_results)
View(roc_results)
roc_results = roc_results[, c(1:8, 11:13, 9:10)]
nA_results = nA_results[, c(1:8, 11:13, 9:10)]
write.table(nA_results, file = 'block_nA_results.txt',row.names = F)
write.table(roc_results, file = 'block_roc_results.txt',row.names = F)
summary(roc_results$rocauc_13)
summary(roc_results$rocauc_12)
summary(roc_results$rocauc_11)
summary(roc_results$rocauc_10)
